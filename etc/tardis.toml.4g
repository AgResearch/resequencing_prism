rootdir = "."
startdir = "."
workdir_is_rootdir = false
input_conditioning = false
in_workflow = true
chunksize = -1  # -1 means it will be calculated to yield <= max_tasks
#samplerate =
#from_record =
#to_record =
dry_run = false
jobtemplatefile = "/dataset/gseq_processing/active/bin/resequencing_prism/etc/slurm_array_job_4g"
#shelltemplatefile =
#runtimeconfigsourcefile =
keep_conditioned_data = false
quiet = false
max_processes = 20
# max_tasks is set low to avoid "over splitting" of the input files - need large chunks as bwa is fast
# small chunks (e.g. 1000000 reads ) end up being inefficient. max_tasks 40 
# would yield a chunk size of maybe 6000000 (for a file of 300m reads or so) 
max_tasks = 40
min_sample_size = 500
hpctype = "slurm"
#batonfile =
valid_command_patterns = ["cat","awk","[t]*blast[nxp]","bwa","bowtie","flexbarf"]
templatedir = "/etc/tardis/templates"
#shell_template_name =
#job_template_name =
#runtime_config_name =
use_session_conda_config = false
#session_conda_config_source =
fast_sequence_input_conditioning = true
